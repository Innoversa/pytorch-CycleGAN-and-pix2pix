diff --git a/CycleGAN.ipynb b/CycleGAN.ipynb
index 9214606..c58df6c 100644
--- a/CycleGAN.ipynb
+++ b/CycleGAN.ipynb
@@ -251,7 +251,7 @@
    "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
   },
   "kernelspec": {
-   "display_name": "Python 3",
+   "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
@@ -265,7 +265,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.10"
+   "version": "3.9.12"
   }
  },
  "nbformat": 4,
diff --git a/environment.yml b/environment.yml
index ecbc55d..a863aa9 100644
--- a/environment.yml
+++ b/environment.yml
@@ -1,4 +1,4 @@
-name: pytorch-CycleGAN-and-pix2pix
+name: cycleGAN
 channels:
 - pytorch
 - defaults
diff --git a/models/base_model.py b/models/base_model.py
index 6de961b..635bdb5 100644
--- a/models/base_model.py
+++ b/models/base_model.py
@@ -32,9 +32,17 @@ class BaseModel(ABC):
         self.opt = opt
         self.gpu_ids = opt.gpu_ids
         self.isTrain = opt.isTrain
-        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')  # get device name: CPU or GPU
-        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)  # save all the checkpoints to save_dir
-        if opt.preprocess != 'scale_width':  # with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.
+        self.device = (
+            torch.device("cuda:{}".format(self.gpu_ids[0]))
+            if self.gpu_ids
+            else torch.device("cpu")
+        )  # get device name: CPU or GPU
+        self.save_dir = os.path.join(
+            opt.checkpoints_dir, opt.name
+        )  # save all the checkpoints to save_dir
+        if (
+            opt.preprocess != "scale_width"
+        ):  # with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.
             torch.backends.cudnn.benchmark = True
         self.loss_names = []
         self.model_names = []
@@ -82,9 +90,11 @@ class BaseModel(ABC):
             opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions
         """
         if self.isTrain:
-            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]
+            self.schedulers = [
+                networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers
+            ]
         if not self.isTrain or opt.continue_train:
-            load_suffix = 'iter_%d' % opt.load_iter if opt.load_iter > 0 else opt.epoch
+            load_suffix = "iter_%d" % opt.load_iter if opt.load_iter > 0 else opt.epoch
             self.load_networks(load_suffix)
         self.print_networks(opt.verbose)
 
@@ -92,7 +102,7 @@ class BaseModel(ABC):
         """Make models eval mode during test time"""
         for name in self.model_names:
             if isinstance(name, str):
-                net = getattr(self, 'net' + name)
+                net = getattr(self, "net" + name)
                 net.eval()
 
     def test(self):
@@ -110,20 +120,20 @@ class BaseModel(ABC):
         pass
 
     def get_image_paths(self):
-        """ Return image paths that are used to load current data"""
+        """Return image paths that are used to load current data"""
         return self.image_paths
 
     def update_learning_rate(self):
         """Update learning rates for all the networks; called at the end of every epoch"""
-        old_lr = self.optimizers[0].param_groups[0]['lr']
+        old_lr = self.optimizers[0].param_groups[0]["lr"]
         for scheduler in self.schedulers:
-            if self.opt.lr_policy == 'plateau':
+            if self.opt.lr_policy == "plateau":
                 scheduler.step(self.metric)
             else:
                 scheduler.step()
 
-        lr = self.optimizers[0].param_groups[0]['lr']
-        print('learning rate %.7f -> %.7f' % (old_lr, lr))
+        lr = self.optimizers[0].param_groups[0]["lr"]
+        print("learning rate %.7f -> %.7f" % (old_lr, lr))
 
     def get_current_visuals(self):
         """Return visualization images. train.py will display these images with visdom, and save the images to a HTML"""
@@ -138,7 +148,9 @@ class BaseModel(ABC):
         errors_ret = OrderedDict()
         for name in self.loss_names:
             if isinstance(name, str):
-                errors_ret[name] = float(getattr(self, 'loss_' + name))  # float(...) works for both scalar tensor and float number
+                errors_ret[name] = float(
+                    getattr(self, "loss_" + name)
+                )  # float(...) works for both scalar tensor and float number
         return errors_ret
 
     def save_networks(self, epoch):
@@ -149,9 +161,9 @@ class BaseModel(ABC):
         """
         for name in self.model_names:
             if isinstance(name, str):
-                save_filename = '%s_net_%s.pth' % (epoch, name)
+                save_filename = "%s_net_%s.pth" % (epoch, name)
                 save_path = os.path.join(self.save_dir, save_filename)
-                net = getattr(self, 'net' + name)
+                net = getattr(self, "net" + name)
 
                 if len(self.gpu_ids) > 0 and torch.cuda.is_available():
                     torch.save(net.module.cpu().state_dict(), save_path)
@@ -163,15 +175,19 @@ class BaseModel(ABC):
         """Fix InstanceNorm checkpoints incompatibility (prior to 0.4)"""
         key = keys[i]
         if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer
-            if module.__class__.__name__.startswith('InstanceNorm') and \
-                    (key == 'running_mean' or key == 'running_var'):
+            if module.__class__.__name__.startswith("InstanceNorm") and (
+                key == "running_mean" or key == "running_var"
+            ):
                 if getattr(module, key) is None:
-                    state_dict.pop('.'.join(keys))
-            if module.__class__.__name__.startswith('InstanceNorm') and \
-               (key == 'num_batches_tracked'):
-                state_dict.pop('.'.join(keys))
+                    state_dict.pop(".".join(keys))
+            if module.__class__.__name__.startswith("InstanceNorm") and (
+                key == "num_batches_tracked"
+            ):
+                state_dict.pop(".".join(keys))
         else:
-            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
+            self.__patch_instance_norm_state_dict(
+                state_dict, getattr(module, key), keys, i + 1
+            )
 
     def load_networks(self, epoch):
         """Load all the networks from the disk.
@@ -181,21 +197,25 @@ class BaseModel(ABC):
         """
         for name in self.model_names:
             if isinstance(name, str):
-                load_filename = '%s_net_%s.pth' % (epoch, name)
+                load_filename = "%s_net_%s.pth" % (epoch, name)
                 load_path = os.path.join(self.save_dir, load_filename)
-                net = getattr(self, 'net' + name)
+                net = getattr(self, "net" + name)
                 if isinstance(net, torch.nn.DataParallel):
                     net = net.module
-                print('loading the model from %s' % load_path)
+                print("loading the model from %s" % load_path)
                 # if you are using PyTorch newer than 0.4 (e.g., built from
                 # GitHub source), you can remove str() on self.device
                 state_dict = torch.load(load_path, map_location=str(self.device))
-                if hasattr(state_dict, '_metadata'):
+                if hasattr(state_dict, "_metadata"):
                     del state_dict._metadata
 
                 # patch InstanceNorm checkpoints prior to 0.4
-                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop
-                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))
+                for key in list(
+                    state_dict.keys()
+                ):  # need to copy keys here because we mutate in loop
+                    self.__patch_instance_norm_state_dict(
+                        state_dict, net, key.split(".")
+                    )
                 net.load_state_dict(state_dict)
 
     def print_networks(self, verbose):
@@ -204,17 +224,20 @@ class BaseModel(ABC):
         Parameters:
             verbose (bool) -- if verbose: print the network architecture
         """
-        print('---------- Networks initialized -------------')
+        print("---------- Networks initialized -------------")
         for name in self.model_names:
             if isinstance(name, str):
-                net = getattr(self, 'net' + name)
+                net = getattr(self, "net" + name)
                 num_params = 0
                 for param in net.parameters():
                     num_params += param.numel()
                 if verbose:
                     print(net)
-                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))
-        print('-----------------------------------------------')
+                print(
+                    "[Network %s] Total number of parameters : %.3f M"
+                    % (name, num_params / 1e6)
+                )
+        print("-----------------------------------------------")
 
     def set_requires_grad(self, nets, requires_grad=False):
         """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
diff --git a/options/base_options.py b/options/base_options.py
index 7a437cc..754a42e 100644
--- a/options/base_options.py
+++ b/options/base_options.py
@@ -6,7 +6,7 @@ import models
 import data
 
 
-class BaseOptions():
+class BaseOptions:
     """This class defines options used during both training and test time.
 
     It also implements several helper functions such as parsing, printing, and saving the options.
@@ -20,41 +20,171 @@ class BaseOptions():
     def initialize(self, parser):
         """Define the common options that are used in both training and test."""
         # basic parameters
-        parser.add_argument('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')
-        parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')
-        parser.add_argument('--use_wandb', action='store_true', help='use wandb')
-        parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')
-        parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')
+        parser.add_argument(
+            "--dataroot",
+            required=True,
+            help="path to images (should have subfolders trainA, trainB, valA, valB, etc)",
+        )
+        parser.add_argument(
+            "--name",
+            type=str,
+            default="experiment_name",
+            help="name of the experiment. It decides where to store samples and models",
+        )
+        parser.add_argument("--use_wandb", action="store_true", help="use wandb")
+        parser.add_argument(
+            "--gpu_ids",
+            type=str,
+            default="0",
+            help="gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU",
+        )
+        parser.add_argument(
+            "--checkpoints_dir",
+            type=str,
+            default="./checkpoints",
+            help="models are saved here",
+        )
         # model parameters
-        parser.add_argument('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')
-        parser.add_argument('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')
-        parser.add_argument('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')
-        parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')
-        parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')
-        parser.add_argument('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')
-        parser.add_argument('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')
-        parser.add_argument('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')
-        parser.add_argument('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')
-        parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')
-        parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')
-        parser.add_argument('--no_dropout', action='store_true', help='no dropout for the generator')
+        parser.add_argument(
+            "--model",
+            type=str,
+            default="cycle_gan",
+            help="chooses which model to use. [cycle_gan | pix2pix | test | colorization]",
+        )
+        parser.add_argument(
+            "--input_nc",
+            type=int,
+            default=3,
+            help="# of input image channels: 3 for RGB and 1 for grayscale",
+        )
+        parser.add_argument(
+            "--output_nc",
+            type=int,
+            default=3,
+            help="# of output image channels: 3 for RGB and 1 for grayscale",
+        )
+        parser.add_argument(
+            "--ngf",
+            type=int,
+            default=64,
+            help="# of gen filters in the last conv layer",
+        )
+        parser.add_argument(
+            "--ndf",
+            type=int,
+            default=64,
+            help="# of discrim filters in the first conv layer",
+        )
+        parser.add_argument(
+            "--netD",
+            type=str,
+            default="basic",
+            help="specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator",
+        )
+        parser.add_argument(
+            "--netG",
+            type=str,
+            default="resnet_9blocks",
+            help="specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]",
+        )
+        parser.add_argument(
+            "--n_layers_D", type=int, default=3, help="only used if netD==n_layers"
+        )
+        parser.add_argument(
+            "--norm",
+            type=str,
+            default="instance",
+            help="instance normalization or batch normalization [instance | batch | none]",
+        )
+        parser.add_argument(
+            "--init_type",
+            type=str,
+            default="normal",
+            help="network initialization [normal | xavier | kaiming | orthogonal]",
+        )
+        parser.add_argument(
+            "--init_gain",
+            type=float,
+            default=0.02,
+            help="scaling factor for normal, xavier and orthogonal.",
+        )
+        parser.add_argument(
+            "--no_dropout", action="store_true", help="no dropout for the generator"
+        )
         # dataset parameters
-        parser.add_argument('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')
-        parser.add_argument('--direction', type=str, default='AtoB', help='AtoB or BtoA')
-        parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')
-        parser.add_argument('--num_threads', default=4, type=int, help='# threads for loading data')
-        parser.add_argument('--batch_size', type=int, default=1, help='input batch size')
-        parser.add_argument('--load_size', type=int, default=286, help='scale images to this size')
-        parser.add_argument('--crop_size', type=int, default=256, help='then crop to this size')
-        parser.add_argument('--max_dataset_size', type=int, default=float("inf"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')
-        parser.add_argument('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')
-        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')
-        parser.add_argument('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')
+        parser.add_argument(
+            "--dataset_mode",
+            type=str,
+            default="unaligned",
+            help="chooses how datasets are loaded. [unaligned | aligned | single | colorization]",
+        )
+        parser.add_argument(
+            "--direction", type=str, default="AtoB", help="AtoB or BtoA"
+        )
+        parser.add_argument(
+            "--serial_batches",
+            action="store_true",
+            help="if true, takes images in order to make batches, otherwise takes them randomly",
+        )
+        parser.add_argument(
+            "--num_threads", default=4, type=int, help="# threads for loading data"
+        )
+        parser.add_argument(
+            "--batch_size", type=int, default=1, help="input batch size"
+        )
+        parser.add_argument(
+            "--load_size", type=int, default=286, help="scale images to this size"
+        )
+        parser.add_argument(
+            "--crop_size", type=int, default=256, help="then crop to this size"
+        )
+        parser.add_argument(
+            "--max_dataset_size",
+            type=int,
+            default=float("inf"),
+            help="Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.",
+        )
+        parser.add_argument(
+            "--preprocess",
+            type=str,
+            default="resize_and_crop",
+            help="scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]",
+        )
+        parser.add_argument(
+            "--no_flip",
+            action="store_true",
+            help="if specified, do not flip the images for data augmentation",
+        )
+        parser.add_argument(
+            "--display_winsize",
+            type=int,
+            default=256,
+            help="display window size for both visdom and HTML",
+        )
         # additional parameters
-        parser.add_argument('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')
-        parser.add_argument('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')
-        parser.add_argument('--verbose', action='store_true', help='if specified, print more debugging information')
-        parser.add_argument('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')
+        parser.add_argument(
+            "--epoch",
+            type=str,
+            default="latest",
+            help="which epoch to load? set to latest to use latest cached model",
+        )
+        parser.add_argument(
+            "--load_iter",
+            type=int,
+            default="0",
+            help="which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]",
+        )
+        parser.add_argument(
+            "--verbose",
+            action="store_true",
+            help="if specified, print more debugging information",
+        )
+        parser.add_argument(
+            "--suffix",
+            default="",
+            type=str,
+            help="customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}",
+        )
         self.initialized = True
         return parser
 
@@ -65,7 +195,9 @@ class BaseOptions():
         in model and dataset classes.
         """
         if not self.initialized:  # check if it has been initialized
-            parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
+            parser = argparse.ArgumentParser(
+                formatter_class=argparse.ArgumentDefaultsHelpFormatter
+            )
             parser = self.initialize(parser)
 
         # get the basic options
@@ -92,39 +224,39 @@ class BaseOptions():
         It will print both current options and default values(if different).
         It will save options into a text file / [checkpoints_dir] / opt.txt
         """
-        message = ''
-        message += '----------------- Options ---------------\n'
+        message = ""
+        message += "----------------- Options ---------------\n"
         for k, v in sorted(vars(opt).items()):
-            comment = ''
+            comment = ""
             default = self.parser.get_default(k)
             if v != default:
-                comment = '\t[default: %s]' % str(default)
-            message += '{:>25}: {:<30}{}\n'.format(str(k), str(v), comment)
-        message += '----------------- End -------------------'
+                comment = "\t[default: %s]" % str(default)
+            message += "{:>25}: {:<30}{}\n".format(str(k), str(v), comment)
+        message += "----------------- End -------------------"
         print(message)
 
         # save to the disk
         expr_dir = os.path.join(opt.checkpoints_dir, opt.name)
         util.mkdirs(expr_dir)
-        file_name = os.path.join(expr_dir, '{}_opt.txt'.format(opt.phase))
-        with open(file_name, 'wt') as opt_file:
+        file_name = os.path.join(expr_dir, "{}_opt.txt".format(opt.phase))
+        with open(file_name, "wt") as opt_file:
             opt_file.write(message)
-            opt_file.write('\n')
+            opt_file.write("\n")
 
     def parse(self):
         """Parse our options, create checkpoints directory suffix, and set up gpu device."""
         opt = self.gather_options()
-        opt.isTrain = self.isTrain   # train or test
+        opt.isTrain = self.isTrain  # train or test
 
         # process opt.suffix
         if opt.suffix:
-            suffix = ('_' + opt.suffix.format(**vars(opt))) if opt.suffix != '' else ''
+            suffix = ("_" + opt.suffix.format(**vars(opt))) if opt.suffix != "" else ""
             opt.name = opt.name + suffix
 
         self.print_options(opt)
 
         # set gpu ids
-        str_ids = opt.gpu_ids.split(',')
+        str_ids = opt.gpu_ids.split(",")
         opt.gpu_ids = []
         for str_id in str_ids:
             id = int(str_id)
diff --git a/train.py b/train.py
index 2852652..7e7a7d5 100644
--- a/train.py
+++ b/train.py
@@ -22,25 +22,33 @@ import time
 from options.train_options import TrainOptions
 from data import create_dataset
 from models import create_model
+import pdb
 from util.visualizer import Visualizer
 
-if __name__ == '__main__':
-    opt = TrainOptions().parse()   # get training options
-    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options
-    dataset_size = len(dataset)    # get the number of images in the dataset.
-    print('The number of training images = %d' % dataset_size)
+if __name__ == "__main__":
+    opt = TrainOptions().parse()  # get training options
+    dataset = create_dataset(
+        opt
+    )  # create a dataset given opt.dataset_mode and other options
+    dataset_size = len(dataset)  # get the number of images in the dataset.
+    print("The number of training images = %d" % dataset_size)
 
-    model = create_model(opt)      # create a model given opt.model and other options
-    model.setup(opt)               # regular setup: load and print networks; create schedulers
-    visualizer = Visualizer(opt)   # create a visualizer that display/save images and plots
-    total_iters = 0                # the total number of training iterations
+    model = create_model(opt)  # create a model given opt.model and other options
+    model.setup(opt)  # regular setup: load and print networks; create schedulers
+    # pdb.set_trace()
+    visualizer = Visualizer(
+        opt
+    )  # create a visualizer that display/save images and plots
+    total_iters = 0  # the total number of training iterations
 
-    for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>
+    for epoch in range(
+        opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1
+    ):  # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>
         epoch_start_time = time.time()  # timer for entire epoch
-        iter_data_time = time.time()    # timer for data loading per iteration
-        epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch
-        visualizer.reset()              # reset the visualizer: make sure it saves the results to HTML at least once every epoch
-        model.update_learning_rate()    # update learning rates in the beginning of every epoch.
+        iter_data_time = time.time()  # timer for data loading per iteration
+        epoch_iter = 0  # the number of training iterations in current epoch, reset to 0 every epoch
+        visualizer.reset()  # reset the visualizer: make sure it saves the results to HTML at least once every epoch
+        model.update_learning_rate()  # update learning rates in the beginning of every epoch.
         for i, data in enumerate(dataset):  # inner loop within one epoch
             iter_start_time = time.time()  # timer for computation per iteration
             if total_iters % opt.print_freq == 0:
@@ -48,30 +56,53 @@ if __name__ == '__main__':
 
             total_iters += opt.batch_size
             epoch_iter += opt.batch_size
-            model.set_input(data)         # unpack data from dataset and apply preprocessing
-            model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
+            model.set_input(data)  # unpack data from dataset and apply preprocessing
+            model.optimize_parameters()  # calculate loss functions, get gradients, update network weights
 
-            if total_iters % opt.display_freq == 0:   # display images on visdom and save images to a HTML file
+            if (
+                total_iters % opt.display_freq == 0
+            ):  # display images on visdom and save images to a HTML file
                 save_result = total_iters % opt.update_html_freq == 0
                 model.compute_visuals()
-                visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)
+                visualizer.display_current_results(
+                    model.get_current_visuals(), epoch, save_result
+                )
 
-            if total_iters % opt.print_freq == 0:    # print training losses and save logging information to the disk
+            if (
+                total_iters % opt.print_freq == 0
+            ):  # print training losses and save logging information to the disk
                 losses = model.get_current_losses()
                 t_comp = (time.time() - iter_start_time) / opt.batch_size
-                visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)
+                visualizer.print_current_losses(
+                    epoch, epoch_iter, losses, t_comp, t_data
+                )
                 if opt.display_id > 0:
-                    visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, losses)
+                    visualizer.plot_current_losses(
+                        epoch, float(epoch_iter) / dataset_size, losses
+                    )
 
-            if total_iters % opt.save_latest_freq == 0:   # cache our latest model every <save_latest_freq> iterations
-                print('saving the latest model (epoch %d, total_iters %d)' % (epoch, total_iters))
-                save_suffix = 'iter_%d' % total_iters if opt.save_by_iter else 'latest'
+            if (
+                total_iters % opt.save_latest_freq == 0
+            ):  # cache our latest model every <save_latest_freq> iterations
+                print(
+                    "saving the latest model (epoch %d, total_iters %d)"
+                    % (epoch, total_iters)
+                )
+                save_suffix = "iter_%d" % total_iters if opt.save_by_iter else "latest"
                 model.save_networks(save_suffix)
 
             iter_data_time = time.time()
-        if epoch % opt.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs
-            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))
-            model.save_networks('latest')
+        if (
+            epoch % opt.save_epoch_freq == 0
+        ):  # cache our model every <save_epoch_freq> epochs
+            print(
+                "saving the model at the end of epoch %d, iters %d"
+                % (epoch, total_iters)
+            )
+            model.save_networks("latest")
             model.save_networks(epoch)
 
-        print('End of epoch %d / %d \t Time Taken: %d sec' % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time))
+        print(
+            "End of epoch %d / %d \t Time Taken: %d sec"
+            % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time)
+        )
